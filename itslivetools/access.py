# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_access.ipynb.

# %% auto 0
__all__ = ['find_granule_by_zone', 'find_granule_by_point', 'read_in_s3']

# %% ../nbs/00_access.ipynb 3
import geopandas as gpd
from shapely.geometry import Point
import os
import numpy as np
import xarray as xr
import rioxarray as rxr
from shapely.geometry import Polygon
from shapely.geometry import Point

import json
import s3fs



# %% ../nbs/00_access.ipynb 5
def find_granule_by_zone(zone):

    catalog = gpd.read_file('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json')

    zone_cat = catalog.loc[catalog['epsg'] == zone]

    return zone_cat

# %% ../nbs/00_access.ipynb 6
def find_granule_by_point(input_point, label='single_point'):
    '''function to find the row (granule) of the itslive catalog that contains a specific point.
    crs needs to be 4326
    '''
    #hard coding this 
    catalog = gpd.read_file('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json')

    #make shapely point of input point
    p = gpd.GeoSeries([Point(input_point[0], input_point[1])],crs='EPSG:4326')
    #make gdf of point
    gdf = gdf = gpd.GeoDataFrame({'label': f'{label}', 
                                  'geometry':p})

    #find row of granule 
    granule = catalog.sjoin(gdf, how='inner')

    url = granule['zarr_url'].values[0]

    return url    

# %% ../nbs/00_access.ipynb 10
def read_in_s3(http_url, chunks='auto'):
    ''' does some string formatting from zarr url and returns xarray dataset
    '''

    s3_url = http_url.replace('http','s3')
    s3_url = s3_url.replace('.s3.amazonaws.com','')

    datacube = xr.open_dataset(s3_url, engine='zarr',
                               storage_options={'anon':True},
                               chunks=chunks)
    return datacube
