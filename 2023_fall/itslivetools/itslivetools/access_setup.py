# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_access_setup.ipynb.

# %% auto 0
__all__ = ['point_to_gdf', 'Glacier', 'remove_empty_timesteps', 'Point', 'calc_min_tbaseline']

# %% ../nbs/02_access_setup.ipynb 4
import warnings
warnings.filterwarnings("ignore")

import os
import json
import salem
import skimage.draw as skdraw

import geopandas as gpd
import shapely
from shapely.geometry import Point
from shapely.geometry import Polygon

import numpy as np
import pandas as pd

import scipy
from scipy.stats import invgauss

import xarray as xr
import rioxarray as rxr
import matplotlib.pyplot as plt
import matplotlib as mpl
import hvplot.pandas
import hvplot.xarray

import geoviews as gv
import geoviews.feature as gf
#import ipywidgets as ipw
import panel as pn

import s3fs
# to get and use geojson datacube catalog
import json
import logging
# for timing data access
import time
import xrspatial
import numpy as np
import pyproj
import s3fs as s3
# for datacube xarray/zarr access
import xarray as xr
import rioxarray as rio
from pyproj import Transformer
# for plotting time series
from shapely import geometry

logging.basicConfig(level=logging.ERROR)
# import pandas as pd

# %% ../nbs/02_access_setup.ipynb 6
from oggm import cfg, utils, graphics
import skimage.draw as skdraw

from oggm import workflow, tasks
from oggm import DEFAULT_BASE_URL
from oggm.shop import its_live, rgitopo, bedtopo


# %% ../nbs/02_access_setup.ipynb 8
def point_to_gdf(point_ls):
        
        d = {'x': point_ls[0],
             'y': point_ls[1]}
        df = pd.DataFrame(d, index=[0])
        gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.x, df.y, crs='EPSG:4326'))
        return gdf
    
class Glacier():
    '''class to hold all data associated with individual glacier
    inputs: name (str), rgi_id (str), working_dir_path (str, where oggm data should be written)
    url (str, url to oggm prepro data), centerline path (str, path to locally stored 
    centerline data (**wanted to have this not rely on local data but haven't gotten that working yet**)
    
    '''
    def __init__(self, name, rgi_id, working_dir_path, url, centerline_path, utm_crs):

        self.name = name
        self.rgi_id = rgi_id
        self.data_url = url
        self.working_dir_path = working_dir_path
        self.centerline_path = centerline_path
        self.g = self.oggm_setup()
        self.utm_crs = utm_crs
        self.centerlines = self.add_centerline()
        self.centerline_main = self.centerlines.loc[self.centerlines['MAIN'] == 1]
        prod = self.add_oggm_gridded_data()
        self.gridded_data = prod[0]
        self.outline = prod[1]
        self.utm_gridded_data = self.reproject_vars()
        self.centerline_gridded_data = self.oggm_gridded_clip2centerline()
        #self.image_pair_centerline = self.add_image_pair_timeseries()
        
    def oggm_setup(self, prepro_level = 2):
        '''method to initialize oggm data that will be used for itslive image pair data 
        preprocessing, and overall data object (thickness, itslive_mosaic, dem, outline) 
        '''
        cfg.initialize(logging_level='WARNING')
        cfg.PARAMS['use_multiprocessing'] = True
        cfg.PATHS['working_dir'] = utils.mkdir(path=self.working_dir_path, reset=False)

        gdirs = workflow.init_glacier_directories(
            self.rgi_id, prepro_base_url = self.data_url,
            from_prepro_level=prepro_level, prepro_border=80)
        list_talks = [
            tasks.glacier_masks,
            its_live.velocity_to_gdir,
            bedtopo.add_consensus_thickness,
        ]
        for task in list_talks:
            workflow.execute_entity_task(task, gdirs)
            
        return gdirs

    def add_centerline(self):
        '''method to add centerline (from local file) as attr to glacier object
        '''
        cl = gpd.read_file(self.centerline_path)
        cl = cl.loc[cl['RGIID'] == self.rgi_id]
        return cl 

    def add_oggm_gridded_data(self):
        '''method to add oggm gridded data + outline to glacier object 
        '''
        g = self.g[0]
        with xr.open_dataset(g.get_filepath('gridded_data')) as ds:
            gridded_data = ds.load()
            gridded_data.rio.write_crs(gridded_data.attrs['pyproj_srs'], inplace=True)
           
        outline = g.read_shapefile('outlines')
        
        for var in gridded_data.data_vars:
            gridded_data[var].rio.write_nodata(-9999., inplace=True)
            gridded_data[var].rio.reproject(self.utm_crs, inplace=True)
        return gridded_data, outline
        
    def reproject_vars(self):
        da_ls = []
        for var in self.gridded_data.data_vars:
            da_ls.append(self.gridded_data[var].rio.reproject(self.utm_crs))
        ds = xr.merge(da_ls)
        return ds
        
    def oggm_gridded_clip2centerline(self):

        vec = self.centerline_main.to_crs(self.utm_crs)
        clip = self.utm_gridded_data.rio.clip(vec.geometry, vec.crs)
        clip.itslive_v.rio.write_nodata(-9999., inplace=True)
        nodata = clip.itslive_v.rio.nodata
        clip = clip.where(clip != nodata)
        clip.itslive_v.rio.write_nodata(nodata, encoded=True, inplace=True)
        return clip
        
    
        
def remove_empty_timesteps(ds):
    ds['step_count'] = (('mid_date'), range(len(ds['mid_date'])))
    ds = ds.swap_dims({'mid_date':'step_count'})
    time_step_keep = list(ds.v.dropna(how='all',dim='step_count').step_count.data)
    ds_subset = ds.where(ds.step_count.isin(time_step_keep), drop=True)
    ds_subset = ds_subset.swap_dims({'step_count':'mid_date'})
    
    return ds_subset
    
class Point():

    def __init__(self, name, rgi_id, glacier_obj, point_coords_latlon, utm_crs, var_ls):

        self.name = name
        self.rgi_id = rgi_id
        self.utm_crs = utm_crs
        self.glacier_gridded_data = glacier_obj.utm_gridded_data
        self.glacier_centerline = glacier_obj.centerline_main
        self.point_latlon = point_coords_latlon
        self.point_gdf = self.point_to_gdf()
        self.point_v = self.point_v_mosaic().itslive_v
        self.datacube_point = self.add_image_pair_point(var_ls)
        self.datacube_sub = self.add_image_pair_subcube(var_ls)
        self.padded_centerline_subcube = self.extract_subcube_along_padded_centerline(name)
        self.TRIM_padded_centerline_subcube = self.subset_ds_by_sensor_baseline('cl')
        self.TRIM_subcube = self.subset_ds_by_sensor_baseline('dc_full')
        self.cube_around_point = self.extract_3x3_cube_around_point(label)
        self.TRIM_cube_around_point = self.subset_ds_by_sensor_baseline('dc_cube')

    def point_to_gdf(self):
        
        d = {'x': self.point_latlon[0],
             'y':self.point_latlon[1]}
        df = pd.DataFrame(d, index=[0])
        gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.x, df.y, crs='EPSG:4326'))
        return gdf

    def point_v_mosaic(self):

        point_utm = self.point_gdf.to_crs(self.utm_crs)
        
        ds_clip = self.glacier_gridded_data.rio.clip(point_utm.geometry, point_utm.crs)
        return ds_clip
            
    def add_image_pair_point(self, var_ls):

        dc = DATACUBETOOLS()
        var_ls = var_ls 
        dc_point_full = dc.get_timeseries_at_point(self.point_latlon, point_epsg_str = '4326', variables = var_ls)
        dc_point = dc_point_full[1]
        crs = f"EPSG:{dc_point.mapping.attrs['spatial_epsg']}"
        dc_point = dc_point.rio.write_crs(crs)
        #dc_point = dc_point.rio.write_nodata(np.nan)
        dc_point = dc_point.dropna(how='any', dim='mid_date')

    
        dc_point['acquisition_date_img1'] = (('mid_date'), pd.to_datetime(dc_point.acquisition_date_img1))
        dc_point['acquisition_date_img2'] = (('mid_date'), pd.to_datetime(dc_point.acquisition_date_img2))
    
        dc_point['img_separation'] = -1*((dc_point.acquisition_date_img1 - dc_point.acquisition_date_img2).astype('timedelta64[D]') / np.timedelta64(1,'D'))

        return dc_point

    def remove_empty_timesteps(ds):
        ds['step_count'] = (('mid_date'), range(len(ds['mid_date'])))
        ds = ds.swap_dims({'mid_date':'step_count'})
        time_step_keep = list(ds.v.dropna(how='all',dim='step_count').step_count.data)
        ds_subset = ds.where(ds.step_count.isin(time_step_keep), drop=True)
        ds_subset = ds_subset.swap_dims({'step_count':'mid_date'})

        return ds_subset
        
    def add_image_pair_subcube(self, var_ls):

        dc = DATACUBETOOLS()
        var_ls = var_ls
        dc_full_sub = dc.get_subcube_around_point(self.point_latlon, point_epsg_str = '4326', variables=var_ls)
        crs = f"EPSG:{dc_full_sub[0].mapping.attrs['spatial_epsg']}"
        dc_sub = dc_full_sub[1]
        dc_sub = dc_sub.rio.write_crs(crs)
        #dc_sub = dc_sub.rio.write_nodata(np.nan)
        dc_sub = dc_sub.dropna(how='all', dim='mid_date')
        dc_sub['acquisition_date_img1'] = (('mid_date'), pd.to_datetime(dc_sub.acquisition_date_img1))
        dc_sub['acquisition_date_img2'] = (('mid_date'), pd.to_datetime(dc_sub.acquisition_date_img2))
    
        dc_sub['img_separation'] = -1*((dc_sub.acquisition_date_img1 - dc_sub.acquisition_date_img2).astype('timedelta64[D]') / np.timedelta64(1,'D'))

        dc_sub = remove_empty_timesteps(dc_sub)
        
        return dc_sub

    def extract_subcube_along_padded_centerline(self, label, pad=200):

        cl = self.glacier_centerline.to_crs(self.utm_crs)
        line = shapely.geometry.LineString(cl.get_coordinates().loc[:,['x','y']].values)
        PAD = pad #meters
        line_buf = gpd.GeoSeries([line], crs=self.utm_crs).buffer(PAD, cap_style=2)
        padded_cl_gdf = gpd.GeoDataFrame({'id':label,
                                  'padding':120}, index=[0], geometry=line_buf)
        glacier_subcube_cl = self.datacube_sub.rio.clip(padded_cl_gdf.geometry, padded_cl_gdf.crs)
        return glacier_subcube_cl

    def subset_ds_by_sensor_baseline(self, format):
        
        min_tb_df = calc_min_tbaseline(self)
        
        #split ds by sensor (sensor options are hardcoded, will need to update when rest of landsat added 
        if format == 'cl': 
            l8 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1 == '8.0',drop=True)
            l9 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1 == '9.0',drop=True)
            s1 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1.isin(['1A','1B']),drop=True)
            s2 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1.isin(['2A','2B']),drop=True)
        elif format == 'dc_full':
            l8 = self.datacube_sub.where(self.datacube_sub.satellite_img1 == '8.0',drop=True)
            l9 = self.datacube_sub.where(self.datacube_sub.satellite_img1 == '9.0',drop=True)
            s1 = self.datacube_sub.where(self.datacube_sub.satellite_img1.isin(['1A','1B']),drop=True)
            s2 = self.datacube_sub.where(self.datacube_sub.satellite_img1.isin(['2A','2B']),drop=True)
            
        elif format == 'dc_cube':
            l8 = self.cube_around_point.where(self.datacube_sub.satellite_img1 == '8.0',drop=True)
            l9 = self.cube_around_point.where(self.datacube_sub.satellite_img1 == '9.0',drop=True)
            s1 = self.cube_around_point.where(self.datacube_sub.satellite_img1.isin(['1A','1B']),drop=True)
            s2 = self.cube_around_point.where(self.datacube_sub.satellite_img1.isin(['2A','2B']),drop=True)
    
        l8_sub = l8.where(l8.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'L8']['min_tb (days)']), drop=True)
        l9_sub = l9.where(l9.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'L9']['min_tb (days)']), drop=True)
        s1_sub = s1.where(s1.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'S1']['min_tb (days)']), drop=True)
        s2_sub = s2.where(s2.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'S2']['min_tb (days)']), drop=True)
        ds_ls = [l8_sub, l9_sub, s1_sub, s2_sub]
        concat_ls = []
        for ds in range(len(ds_ls)):
            if len(ds_ls[ds].mid_date) > 0:
                concat_ls.append(ds_ls[ds])
        combine = xr.concat(concat_ls, dim='mid_date')
        combine = combine.sortby(combine.mid_date)
        return combine
        
    def extract_3x3_cube_around_point(self, label):
    
        padded_point = gpd.GeoDataFrame({'id':label}, 
                                index=[0],
                                geometry = self.point_gdf.to_crs('EPSG:32643').buffer(distance=200))
        dc = self.datacube_sub.rio.clip(padded_point.geometry, padded_point.crs)

        return dc

#class FilteredPoint(Point):
#    def __init__(name, id, glacier_obj, point_coords_latlon, utm_crs, var_ls):
#        super().__init__(name, id, glacier_obj, point_coords_latlon, utm_crs)
        
#        self.point_gdf = self.point_to_gdf()
#        self.TRIM_padded_centerline_subcube = self.subset_ds_by_sensor_baseline('cl')
#        self.TRIM_subcube = self.subset_ds_by_sensor_baseline('dc_full')
#        self.TRIM_3cube = self.subset_ds_by_sensor_baseline('dc_cube')


                         
def calc_min_tbaseline(Point):
    med_v = Point.point_v.data[0][0]
    gsd_s2, gsd_l8, gsd_s1, gsd_l9 = 10,15, 10, 15
    name_ls = ['S2','L8','S1', 'L9']
    gsd_ls = [gsd_s2, gsd_l8, gsd_s1, gsd_l9]
    sensor_str_ls = [['2A','2B'], '8.0',['1A','1B'],'9.0']
    min_tb_ls = []
    for element in range(len(gsd_ls)):
        min_tb = ((gsd_ls[element]*2)/med_v)*365
        min_tb_ls.append(min_tb)
        #print(min_tb, ' days')

    min_tb_dict= {'sensor':name_ls, 
                  'gsd': gsd_ls, 
                  'min_tb (days)': min_tb_ls,
                 'sensor_str':sensor_str_ls}
    df = pd.DataFrame(min_tb_dict)
    return df
