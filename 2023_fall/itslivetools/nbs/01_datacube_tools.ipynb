{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a308e7-8a20-4a18-8998-9e7afb0fdadc",
   "metadata": {},
   "source": [
    "# datacube_tools\n",
    "- from ITS_LIVE developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f132561-3679-4d5f-aa05-507c73c93338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datacube_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b061ed5a-be47-429b-9c00-094f91e58963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev\n",
    "from nbdev import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a8718d-a99a-4f79-8025-997d53735b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "# to get and use geojson datacube catalog\n",
    "import json\n",
    "import logging\n",
    "# for timing data access\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import s3fs as s3\n",
    "# for datacube xarray/zarr access\n",
    "import xarray as xr\n",
    "from pyproj import Transformer\n",
    "# for plotting time series\n",
    "from shapely import geometry\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# class to throw time series lookup errors\n",
    "class timeseriesException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DATACUBETOOLS:\n",
    "    \"\"\"\n",
    "    class to encapsulate discovery and interaction with ITS_LIVE (its-live.jpl.nasa.gov) datacubes on AWS s3\n",
    "    \"\"\"\n",
    "\n",
    "    VELOCITY_DATA_ATTRIBUTION = \"\"\" \\nITS_LIVE velocity data\n",
    "    (<a href=\"https://its-live.jpl.nasa.gov\">ITS_LIVE</a>) with funding provided by NASA MEaSUREs.\\n\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_catalog=\"all\"):\n",
    "        \"\"\"\n",
    "        tools for accessing ITS_LIVE glacier velocity datacubes in S3\n",
    "        __init__ reads in the geojson catalog of datacubes and creates list .open_cubes\n",
    "        \"\"\"\n",
    "        # the URL for the current datacube catalog GeoJSON file - set up as dictionary to allow other catalogs for testing\n",
    "        self.catalog = {\n",
    "            \"all\": \"s3://its-live-data/datacubes/catalog_v02.json\",\n",
    "        }\n",
    "\n",
    "        self.transformer_3031 = Transformer.from_crs(4326, 3031, always_xy=True)\n",
    "\n",
    "        self.elevation_dataset = xr.open_dataset(\n",
    "            \"s3://its-live-data/elevation/v01/ANT_G1920V01_GroundedIceHeight.zarr\",\n",
    "            engine=\"zarr\",\n",
    "            storage_options={\"anon\": True},\n",
    "        )\n",
    "\n",
    "        # S3fs used to access cubes in python\n",
    "        self._s3fs = s3.S3FileSystem(anon=True)\n",
    "        # keep track of open cubes so that we don't re-read xarray metadata and dimension vectors\n",
    "        self.open_cubes = {}\n",
    "        self._current_catalog = use_catalog\n",
    "        with self._s3fs.open(self.catalog[use_catalog], \"r\") as incubejson:\n",
    "            self._json_all = json.load(incubejson)\n",
    "        self.json_catalog = self._json_all\n",
    "\n",
    "    def load_elevation_timeseries(self, lon, lat):\n",
    "\n",
    "        x_projected, y_projected = self.transformer_3031.transform(lon, lat)\n",
    "        ts = (\n",
    "            self.elevation_dataset[\"dh\"]\n",
    "            .sel(x=x_projected, y=y_projected, method=\"nearest\")\n",
    "            .to_dataframe()\n",
    "            .dropna()\n",
    "        )\n",
    "        return ts[\"dh\"]\n",
    "\n",
    "    def find_datacube_catalog_entry_for_point(self, point_xy, point_epsg_str):\n",
    "        \"\"\"\n",
    "        find catalog feature that contains the point_xy [x,y] in projection point_epsg_str (e.g. '3413')\n",
    "        returns the catalog feature and the point_tilexy original point coordinates reprojected into the datacube's native projection\n",
    "        (cubefeature, point_tilexy)\n",
    "        \"\"\"\n",
    "        if point_epsg_str != \"4326\":\n",
    "            # point not in lon,lat, set up transformation and convert it to lon,lat (epsg:4326)\n",
    "            # because the features in the catalog GeoJSON are polygons in 4326\n",
    "            inPROJtoLL = pyproj.Transformer.from_proj(\n",
    "                f\"epsg:{point_epsg_str}\", \"epsg:4326\", always_xy=True\n",
    "            )\n",
    "            pointll = inPROJtoLL.transform(*point_xy)\n",
    "        else:\n",
    "            # point already lon,lat\n",
    "            pointll = point_xy\n",
    "\n",
    "        # create Shapely point object for inclusion test\n",
    "        point = geometry.Point(*pointll)  # point.coords.xy\n",
    "\n",
    "        # find datacube outline that contains this point in geojson index file\n",
    "        cubefeature = None\n",
    "\n",
    "        for f in self.json_catalog[\"features\"]:\n",
    "            polygeom = geometry.shape(f[\"geometry\"])\n",
    "            if polygeom.contains(point):\n",
    "                cubefeature = f\n",
    "                break\n",
    "\n",
    "        if cubefeature:\n",
    "            # find point x and y in cube native epsg if not already in that projection\n",
    "            if point_epsg_str == str(cubefeature[\"properties\"][\"epsg\"]):\n",
    "                point_cubexy = point_xy\n",
    "            else:\n",
    "                inPROJtoTilePROJ = pyproj.Transformer.from_proj(\n",
    "                    f\"epsg:{point_epsg_str}\",\n",
    "                    f\"EPSG:{cubefeature['properties']['epsg']}\",\n",
    "                    always_xy=True,\n",
    "                )\n",
    "                point_cubexy = inPROJtoTilePROJ.transform(*point_xy)\n",
    "\n",
    "            print(\n",
    "                f\"original xy {point_xy} {point_epsg_str} maps to datacube {point_cubexy} \"\n",
    "                f\"EPSG:{cubefeature['properties']['epsg']}\"\n",
    "            )\n",
    "\n",
    "            # now test if point is in xy box for cube (should be most of the time; could fail\n",
    "            # because of boundary curvature 4326 box defined by lon,lat corners but point needs to be in box defined in cube's projection)\n",
    "            #\n",
    "            point_cubexy_shapely = geometry.Point(*point_cubexy)\n",
    "            polygeomxy = geometry.shape(cubefeature[\"properties\"][\"geometry_epsg\"])\n",
    "            if not polygeomxy.contains(point_cubexy_shapely):\n",
    "                # first find cube proj bounding box\n",
    "                dcbbox = np.array(\n",
    "                    cubefeature[\"properties\"][\"geometry_epsg\"][\"coordinates\"][0]\n",
    "                )\n",
    "                minx = np.min(dcbbox[:, 0])\n",
    "                maxx = np.max(dcbbox[:, 0])\n",
    "                miny = np.min(dcbbox[:, 1])\n",
    "                maxy = np.max(dcbbox[:, 1])\n",
    "\n",
    "                # point is in lat lon box, but not in cube-projection's box\n",
    "                # try once more to find proper cube by using a new point in cube projection moved 10 km farther from closest\n",
    "                # boundary in cube projection; use new point's lat lon to search for new cube - test if old point is in that\n",
    "                # new cube's projection box, otherwise ...\n",
    "                # this next section tries one more time to find new feature after offsetting point farther outside box of\n",
    "                # first cube, in cube projection, to deal with curvature of lat lon box edges in different projections\n",
    "                #\n",
    "                # point in ll box but not cube_projection box, move point in cube projection\n",
    "                # 10 km farther outside box, find new ll value for point, find new feature it is in,\n",
    "                # and check again if original point falls in this new cube's\n",
    "                # move coordinate of point outside this box farther out by 10 km\n",
    "\n",
    "                newpoint_cubexy = list(point_cubexy)\n",
    "                if point_cubexy[1] < miny:\n",
    "                    newpoint_cubexy[1] -= 10000.0\n",
    "                elif point_cubexy[1] > maxy:\n",
    "                    newpoint_cubexy[1] += 10000.0\n",
    "                elif point_cubexy[0] < minx:\n",
    "                    newpoint_cubexy[0] -= 10000.0\n",
    "                elif point_cubexy[0] > maxx:\n",
    "                    newpoint_cubexy[0] += 10000.0\n",
    "                else:\n",
    "                    # no change has been made to newpoint_cubexy because\n",
    "                    # user has chosen a point exactly on the boundary, move it 1 m into the box...\n",
    "                    logging.info(\n",
    "                        \"user has chosen a point exactly on the boundary, move it 1 m into the box...\"\n",
    "                    )\n",
    "                    if point_cubexy[1] == miny:\n",
    "                        newpoint_cubexy[1] += 1.0\n",
    "                    elif point_cubexy[1] == maxy:\n",
    "                        newpoint_cubexy[1] -= 1.0\n",
    "                    elif point_cubexy[0] == minx:\n",
    "                        newpoint_cubexy[0] += 1.0\n",
    "                    elif point_cubexy[0] == maxx:\n",
    "                        newpoint_cubexy[0] -= 1.0\n",
    "\n",
    "                # now reproject this point to lat lon and look for new feature\n",
    "                if \"data_epsg\" in cubefeature[\"properties\"]:\n",
    "                    epsg_source = cubefeature[\"properties\"][\"data_epsg\"]\n",
    "                elif \"projection\" in cubefeaturea[\"properties\"]:\n",
    "                    epsg_source = cubefeature[\"properties\"][\"projection\"]\n",
    "                else:\n",
    "                    epsg_source = None\n",
    "\n",
    "                if epsg_source is None:\n",
    "                    print(\"Source projection not found\")\n",
    "                    return None\n",
    "\n",
    "                cubePROJtoLL = pyproj.Transformer.from_proj(\n",
    "                    f'{cubefeature[\"properties\"][\"data_epsg\"]}',\n",
    "                    \"epsg:4326\",\n",
    "                    always_xy=True,\n",
    "                )\n",
    "                newpointll = cubePROJtoLL.transform(*newpoint_cubexy)\n",
    "\n",
    "                # create Shapely point object for inclusion test\n",
    "                newpoint = geometry.Point(*newpointll)\n",
    "\n",
    "                # find datacube outline that contains this point in geojson index file\n",
    "                newcubefeature = None\n",
    "\n",
    "                for f in self.json_catalog[\"features\"]:\n",
    "                    polygeom = geometry.shape(f[\"geometry\"])\n",
    "                    if polygeom.contains(newpoint):\n",
    "                        newcubefeature = f\n",
    "                        break\n",
    "\n",
    "                if newcubefeature:\n",
    "                    # if new feature found, see if original (not offset) point is in this new cube's cube-projection bounding box\n",
    "                    # find point x and y in cube native epsg if not already in that projection\n",
    "                    if (\n",
    "                        cubefeature[\"properties\"][\"data_epsg\"]\n",
    "                        == newcubefeature[\"properties\"][\"data_epsg\"]\n",
    "                    ):\n",
    "                        point_cubexy = newpoint_cubexy\n",
    "                    else:\n",
    "                        # project original point in this new cube's projection\n",
    "                        inPROJtoTilePROJ = pyproj.Transformer.from_proj(\n",
    "                            f\"epsg:{point_epsg_str}\",\n",
    "                            newcubefeature[\"properties\"][\"data_epsg\"],\n",
    "                            always_xy=True,\n",
    "                        )\n",
    "                        point_cubexy = inPROJtoTilePROJ.transform(*point_xy)\n",
    "\n",
    "                    logging.info(\n",
    "                        f\"try 2 original xy {point_xy} {point_epsg_str} with offset maps to new datacube {point_cubexy} \"\n",
    "                        f\" {newcubefeature['properties']['data_epsg']}\"\n",
    "                    )\n",
    "\n",
    "                    # now test if point is in xy box for cube (should be most of the time;\n",
    "                    #\n",
    "                    point_cubexy_shapely = geometry.Point(*point_cubexy)\n",
    "                    polygeomxy = geometry.shape(\n",
    "                        newcubefeature[\"properties\"][\"geometry_epsg\"]\n",
    "                    )\n",
    "                    if not polygeomxy.contains(point_cubexy_shapely):\n",
    "                        # point is in lat lon box, but not in cube-projection's box\n",
    "                        # try once more to find proper cube by using a new point in cube projection moved 10 km farther from closest\n",
    "                        # boundary in cube projection; use new point's lat lon to search for new cube - test if old point is in that\n",
    "                        # new cube's projection box, otherwise fail...\n",
    "\n",
    "                        raise timeseriesException(\n",
    "                            f\"point is in lat,lon box but not {cubefeature['properties']['data_epsg']} box!! even after offset\"\n",
    "                        )\n",
    "                    else:\n",
    "                        return (newcubefeature, point_cubexy)\n",
    "\n",
    "            else:\n",
    "                return (cubefeature, point_cubexy)\n",
    "\n",
    "        else:\n",
    "            print(f\"No data for point (lon,lat) {pointll}\")\n",
    "            return (None, None)\n",
    "\n",
    "    def get_timeseries_at_point(self, point_xy, point_epsg_str, variables=[\"v\"]):\n",
    "        \"\"\"pulls time series for a point (closest ITS_LIVE point to given location):\n",
    "        - calls find_datacube to determine which S3-based datacube the point is in,\n",
    "        - opens that xarray datacube - which is also added to the open_cubes list, so that it won't need to be reopened (which can take O(5 sec) ),\n",
    "        - extracts time series at closest grid cell to the original point\n",
    "            (time_series.x and time_series.y contain x and y coordinates of ITS_LIVE grid cell in datacube projection)\n",
    "\n",
    "        returns(\n",
    "            - xarray of open full cube (not loaded locally, but coordinate vectors and attributes for full cube are),\n",
    "            - time_series (as xarray dataset with all requested variables, that is loaded locally),\n",
    "            - original point xy in datacube's projection\n",
    "            )\n",
    "\n",
    "        NOTE - returns an xarray Dataset (not just a single xarray DataArray) - time_series.v or time_series['v'] is speed\n",
    "        \"\"\"\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        cube_feature, point_cubexy = self.find_datacube_catalog_entry_for_point(\n",
    "            point_xy, point_epsg_str\n",
    "        )\n",
    "\n",
    "        if cube_feature is None:\n",
    "            return (None, None, None)\n",
    "\n",
    "        # for zarr store modify URL for use in boto open - change http: to s3: and lose s3.amazonaws.com\n",
    "        incubeurl = (\n",
    "            cube_feature[\"properties\"][\"zarr_url\"]\n",
    "            .replace(\"http:\", \"s3:\")\n",
    "            .replace(\".s3.amazonaws.com\", \"\")\n",
    "        )\n",
    "\n",
    "        # if we have already opened this cube, don't open it again\n",
    "        if len(self.open_cubes) > 0 and incubeurl in self.open_cubes.keys():\n",
    "            ins3xr = self.open_cubes[incubeurl]\n",
    "        else:\n",
    "            ins3xr = xr.open_dataset(\n",
    "                incubeurl, engine=\"zarr\", storage_options={\"anon\": True}\n",
    "            )\n",
    "            self.open_cubes[incubeurl] = ins3xr\n",
    "\n",
    "        # find time series at the closest grid cell\n",
    "        # NOTE - returns an xarray Dataset - pt_dataset.v is speed...\n",
    "        pt_datset = ins3xr[variables].sel(\n",
    "            x=point_cubexy[0], y=point_cubexy[1], method=\"nearest\"\n",
    "        )\n",
    "\n",
    "        logging.info(\n",
    "            f\"xarray open - elapsed time: {(time.time()-start):10.2f}\", flush=True\n",
    "        )\n",
    "\n",
    "        # pull data to local machine\n",
    "        pt_datset.load()\n",
    "\n",
    "        # print(\n",
    "        #     f\"time series loaded {[f'{x}: {pt_datset[x].shape[0]}' for x in variables]} points - elapsed time: {(time.time()-start):10.2f}\",\n",
    "        #     flush=True,\n",
    "        # )\n",
    "        # end for zarr store\n",
    "\n",
    "        return (ins3xr, pt_datset, point_cubexy)\n",
    "\n",
    "    def set_mapping_for_small_cube_from_larger_one(self, smallcube, largecube):\n",
    "        \"\"\"when a subset is pulled from an ITS_LIVE datacube, a new geotransform needs to be\n",
    "        figured out from the smallcube's x and y coordinates and stored in the GeoTransform attribute\n",
    "        of the mapping variable (which also needs to be copied from the original cube)\n",
    "        \"\"\"\n",
    "        largecube_gt = [float(x) for x in largecube.mapping.GeoTransform.split(\" \")]\n",
    "        smallcube_gt = largecube_gt  # need to change corners still\n",
    "        # find UL corner of UL pixel (x and y are pixel center coordinates)\n",
    "        smallcube_gt[0] = smallcube.x.min().item() - (\n",
    "            smallcube_gt[1] / 2.0\n",
    "        )  # set new ul x value\n",
    "        smallcube_gt[3] = smallcube.y.max().item() - (\n",
    "            smallcube_gt[5] / 2.0\n",
    "        )  # set new ul y value\n",
    "        smallcube[\n",
    "            \"mapping\"\n",
    "        ] = largecube.mapping  # still need to add new GeoTransform as string\n",
    "        smallcube.mapping[\"GeoTransform\"] = \" \".join([str(x) for x in smallcube_gt])\n",
    "        return\n",
    "\n",
    "    def get_subcube_around_point(\n",
    "        self, point_xy, point_epsg_str, half_distance=5000.0, variables=[\"v\"]\n",
    "    ):\n",
    "        \"\"\"pulls subset of cube within half_distance of point (unless edge of cube is included) containing specified variables:\n",
    "        - calls find_datacube to determine which S3-based datacube the point is in,\n",
    "        - opens that xarray datacube - which is also added to the open_cubes list, so that it won't need to be reopened (which can take O(5 sec) ),\n",
    "        - extracts smaller cube containing full time series of specified variables\n",
    "\n",
    "        returns(\n",
    "            - xarray of open full cube (not loaded locally, but coordinate vectors and attributes for full cube are),\n",
    "            - smaller cube as xarray,\n",
    "            - original point xy in datacube's projection\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        cube_feature, point_cubexy = self.find_datacube_catalog_entry_for_point(\n",
    "            point_xy, point_epsg_str\n",
    "        )\n",
    "\n",
    "        # for zarr store modify URL for use in boto open - change http: to s3: and lose s3.amazonaws.com\n",
    "        incubeurl = (\n",
    "            cube_feature[\"properties\"][\"zarr_url\"]\n",
    "            .replace(\"http:\", \"s3:\")\n",
    "            .replace(\".s3.amazonaws.com\", \"\")\n",
    "        )\n",
    "\n",
    "        # if we have already opened this cube, don't open it again\n",
    "        if len(self.open_cubes) > 0 and incubeurl in self.open_cubes.keys():\n",
    "            ins3xr = self.open_cubes[incubeurl]\n",
    "        else:\n",
    "            ins3xr = xr.open_dataset(\n",
    "                incubeurl, engine=\"zarr\", storage_options={\"anon\": True}\n",
    "            )\n",
    "            self.open_cubes[incubeurl] = ins3xr\n",
    "\n",
    "        pt_tx, pt_ty = point_cubexy\n",
    "        lx = ins3xr.coords[\"x\"]\n",
    "        ly = ins3xr.coords[\"y\"]\n",
    "\n",
    "        start = time.time()\n",
    "        small_ins3xr = (\n",
    "            ins3xr[variables]\n",
    "            .loc[\n",
    "                dict(\n",
    "                    x=lx[(lx > pt_tx - half_distance) & (lx < pt_tx + half_distance)],\n",
    "                    y=ly[(ly > pt_ty - half_distance) & (ly < pt_ty + half_distance)],\n",
    "                )\n",
    "            ]\n",
    "            .load()\n",
    "        )\n",
    "        print(f\"subset and load at {time.time() - start:6.2f} seconds\", flush=True)\n",
    "\n",
    "        # now fix the CF compliant geolocation/mapping of the smaller cube\n",
    "        self.set_mapping_for_small_cube_from_larger_one(small_ins3xr, ins3xr)\n",
    "\n",
    "        return (ins3xr, small_ins3xr, point_cubexy)\n",
    "\n",
    "    def get_subcube_for_bounding_box(self, bbox, bbox_epsg_str, variables=[\"v\"]):\n",
    "        \"\"\"pulls subset of cube within bbox (unless edge of cube is included) containing specified variables:\n",
    "        - calls find_datacube to determine which S3-based datacube the bbox central point is in,\n",
    "        - opens that xarray datacube - which is also added to the open_cubes list, so that it won't need to be reopened (which can take O(5 sec) ),\n",
    "        - extracts smaller cube containing full time series of specified variables\n",
    "\n",
    "        bbox = [ minx, miny, maxx, maxy ] in bbox_epsg_str meters\n",
    "        bbox_epsg_str = '3413', '32607', '3031', ... (EPSG:xxxx) projection identifier\n",
    "        variables = [ 'v', 'vx', 'vy', ...] variables in datacube - note 'mapping' is returned by default, with updated geotransform attribute for the new subcube size\n",
    "\n",
    "        returns(\n",
    "            - xarray of open full cube (not loaded locally, but coordinate vectors and attributes for full cube are),\n",
    "            - smaller cube as xarray (loaded to memory),\n",
    "            - original bbox central point xy in datacube's projection\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        #\n",
    "        # derived from point/distance (get_subcube_around_point) so first iteration uses central point to look up datacube to open\n",
    "        # subcube will still be clipped at datacube edge if bbox extends to other datacubes - in future maybe return subcubes from each?\n",
    "        #\n",
    "        # bbox is probably best expressed in datacube epsg - will fail if different...  in future, deal with this some other way.\n",
    "        #\n",
    "\n",
    "        bbox_minx, bbox_miny, bbox_maxx, bbox_maxy = bbox\n",
    "        bbox_centrer_point_xy = [\n",
    "            (bbox_minx + bbox_maxx) / 2.0,\n",
    "            (bbox_miny + bbox_maxy) / 2.0,\n",
    "        ]\n",
    "        (\n",
    "            cube_feature,\n",
    "            bbox_centrer_point_cubexy,\n",
    "        ) = self.find_datacube_catalog_entry_for_point(\n",
    "            bbox_centrer_point_xy, bbox_epsg_str\n",
    "        )\n",
    "\n",
    "        if cube_feature[\"properties\"][\"data_epsg\"].split(\":\")[-1] != bbox_epsg_str:\n",
    "            print(\n",
    "                f'bbox is in epsg:{bbox_epsg_str}, should be in datacube {cube_feature[\"properties\"][\"data_epsg\"]}'\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # for zarr store modify URL for use in boto open - change http: to s3: and lose s3.amazonaws.com\n",
    "        incubeurl = (\n",
    "            cube_feature[\"properties\"][\"zarr_url\"]\n",
    "            .replace(\"http:\", \"s3:\")\n",
    "            .replace(\".s3.amazonaws.com\", \"\")\n",
    "        )\n",
    "\n",
    "        # if we have already opened this cube, don't open it again\n",
    "        if len(self.open_cubes) > 0 and incubeurl in self.open_cubes.keys():\n",
    "            ins3xr = self.open_cubes[incubeurl]\n",
    "        else:\n",
    "            # open zarr format xarray datacube on AWS S3\n",
    "            ins3xr = xr.open_dataset(\n",
    "                incubeurl, engine=\"zarr\", storage_options={\"anon\": True}\n",
    "            )\n",
    "            self.open_cubes[incubeurl] = ins3xr\n",
    "\n",
    "        lx = ins3xr.coords[\"x\"]\n",
    "        ly = ins3xr.coords[\"y\"]\n",
    "\n",
    "        start = time.time()\n",
    "        small_ins3xr = (\n",
    "            ins3xr[variables]\n",
    "            .loc[\n",
    "                dict(\n",
    "                    x=lx[(lx >= bbox_minx) & (lx <= bbox_maxx)],\n",
    "                    y=ly[(ly >= bbox_miny) & (ly <= bbox_maxy)],\n",
    "                )\n",
    "            ]\n",
    "            .load()\n",
    "        )\n",
    "        print(f\"subset and load at {time.time() - start:6.2f} seconds\", flush=True)\n",
    "\n",
    "        # now fix the CF compliant geolocation/mapping of the smaller cube\n",
    "        self.set_mapping_for_small_cube_from_larger_one(small_ins3xr, ins3xr)\n",
    "\n",
    "        return (ins3xr, small_ins3xr, bbox_centrer_point_cubexy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab78b7-599a-420a-8f20-104768e782f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
